# -*- coding: utf-8 -*-
"""backup.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EnFO-OjW0PZn4T2Db_3bbGI_zbKP7F5I
"""

import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import re

df = pd.read_csv('IMDB Dataset.csv')

df.shape

df.head()

df.info()

"""## Data Cleaning"""

# Mengubah nilai positive menjadi 1 dan negative menjadi 0
df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def text_cleaning(text):
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    word_tokens = word_tokenize(text)
    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]
    lemmatized_words = [lemmatizer.lemmatize(word, pos="v") for word in filtered_sentence]
    text = " ".join(lemmatized_words)

    return text

df.review = df['review'].apply(text_cleaning)

df.head()

def sum_word(sentence):
  words= sentence.split()
  word_count = len(words)
  return word_count

"""Saya membuat kolom count_word untuk menentukan panjang maksimum setiap review"""

df['count_word']= df['review'].apply(sum_word)

df.describe()

unique_words = set(" ".join(df['review']).split())
print("Jumlah kata unik dalam dataset:", len(unique_words))

"""## Feature Selection"""

from sklearn.model_selection import train_test_split
X = df['review'].values
y = df['sentiment'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Tokenization"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

max_words = 25000  # Jumlah kata unik yang akan diambil
max_len = 100      # Panjang maksimum setiap review, saya mengambil 100 karena median count word adalah 98 saya bulatkan menjadi 100

tokenizer = Tokenizer(num_words=max_words, oov_token="<oov>")
tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_test)

train_sequences = tokenizer.texts_to_sequences(X_train)
test_sequences = tokenizer.texts_to_sequences(X_test)

train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')
test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')

"""## Modeling"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, GlobalMaxPool1D

model = Sequential([
    Embedding(input_dim=max_words, output_dim=16, input_length=max_len),
    Bidirectional(LSTM(16,dropout=0.5, return_sequences = True)),
    GlobalMaxPool1D(),
    Dropout(0.5),
    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
model.compile(
    loss='binary_crossentropy',
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)

model.summary()

from tensorflow.keras.callbacks import Callback, EarlyStopping
class MyCallback(Callback):
   def on_epoch_end(self, epoch, logs={}):
      if (logs.get('accuracy') >= 0.90) and (logs.get('val_accuracy') >= 0.90):
          print("\nReached 90% accuracy, stopping training")
          self.model.stop_training = True

early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)

history= model.fit(train_padded, y_train,
          epochs=20,
          batch_size=100,
          validation_data=(test_padded, y_test),
          verbose=2,
          callbacks=[MyCallback(), early_stopping]
          )

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train','test'], loc='upper right')
plt.show()

"""### Testing"""

reviews = ["this movie is really cool!", "i dont like this movie because it's boring"]
sequences = tokenizer.texts_to_sequences(reviews)
reviews_padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')

pred = model.predict(reviews_padded)
for review, prediction in zip(reviews, pred):
    sentiment = 'positive' if prediction >= 0.5 else 'negative'
    print(f"Review: {review}\nPredicted Sentiment: {sentiment}\n")